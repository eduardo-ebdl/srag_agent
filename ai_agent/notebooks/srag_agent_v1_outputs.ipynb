{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef39dfe-0f9e-4b3b-af1b-4473f27a1162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas de Orquestração (LangChain/Graph), Conectores (Databricks) e Pesquisa (Tavily).\n",
    "%pip install -U langchain langchain-community langchain-core langgraph databricks-langchain tavily-python matplotlib pandas mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70594b0-22fb-4dc1-b2f9-89f02c6d02b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "# Bibliotecas padrão\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Dados e visualização\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# LangChain e LLMs\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Serviços externos e ML\n",
    "from tavily import TavilyClient\n",
    "import mlflow\n",
    "\n",
    "# Configuração de Log\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"\n",
    "    Logger padronizado para rastreabilidade de execução.\n",
    "    Formato: [YYYY-MM-DD HH:MM:SS] [LEVEL] Icon Mensagem\n",
    "    \"\"\"\n",
    "    # Configuração de Fuso Horário (BRT)\n",
    "    fuso_br = timezone(timedelta(hours=-3))\n",
    "    timestamp = datetime.now(fuso_br).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Ícones visuais para facilitar leitura rápida dos logs\n",
    "    icons = {\n",
    "        \"INFO\": \"ℹ️\", \n",
    "        \"WARN\": \"⚠️\", \n",
    "        \"ERROR\": \"❌\", \n",
    "        \"SUCCESS\": \"✅\", \n",
    "        \"SYSTEM\": \"⚙️\",\n",
    "        \"TOOL\": \"\uD83D\uDEE0️\",\n",
    "        \"AI\": \"\uD83E\uDD16\"\n",
    "    }\n",
    "    icon = icons.get(level, \"\")\n",
    "    \n",
    "    print(f\"[{timestamp}] [{level}] {icon} {msg}\")\n",
    "\n",
    "log(\"Bibliotecas importadas e Logger configurado com sucesso.\", \"SYSTEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf0b0c2-cbad-4f5a-8726-93052e2b8351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define o experimento\n",
    "\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/srag_agent_monitoring\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "# Habilita o rastreamento automático para LangChain (capturar inputs, outputs, traces)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "log(f\"MLOps Ativado. Experiment Path: {experiment_path}\", \"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c79805a-fffb-4e07-b9af-d7b20d7ca16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquitetura dos dados\n",
    "\n",
    "CATALOG = \"srag_prod\" \n",
    "SCHEMA = \"gold\"       \n",
    "VOLUME_NAME = \"volume_imagens\" # Storage para arquivos não estruturados (png)\n",
    "\n",
    "# Garante a existência do volume para persistência dos gráficos gerados pelo agente\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME_NAME}\")\n",
    "\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "TABLE_DAILY = f\"{CATALOG}.{SCHEMA}.gold_srag_daily\"\n",
    "TABLE_MONTHLY = f\"{CATALOG}.{SCHEMA}.gold_srag_monthly\"\n",
    "\n",
    "log(f\"Ambiente de Dados Configurado.\", \"SYSTEM\")\n",
    "log(f\"Caminho do Volume: {VOLUME_PATH}\", \"INFO\")\n",
    "log(f\"Tabelas Mapeadas: {TABLE_DAILY}, {TABLE_MONTHLY}\", \"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba5a11f-8e3e-4b63-9541-91e3e6397da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validação dos dados\n",
    "\n",
    "def run_quality_gate():\n",
    "    \"\"\"\n",
    "    Valida os dados da tabela Gold. Se falhar, interrompe o notebook.\n",
    "    Regras:\n",
    "    1. Apenas anos 2024 e 2025.\n",
    "    2. Sem casos negativos.\n",
    "    3. Sem datas nulas.\n",
    "    \"\"\"\n",
    "    log(\"Executando Validação de Qualidade de Dados (Data Quality Gate)...\", \"SYSTEM\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # A função 'sum' conta quantas linhas violam cada regra\n",
    "    query_check = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_linhas,\n",
    "            SUM(CASE WHEN year(data_referencia) NOT IN (2024, 2025) THEN 1 ELSE 0 END) as erro_anos,\n",
    "            SUM(CASE WHEN total_casos < 0 THEN 1 ELSE 0 END) as erro_negativos,\n",
    "            SUM(CASE WHEN data_referencia IS NULL THEN 1 ELSE 0 END) as erro_nulos\n",
    "        FROM {TABLE_DAILY}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coleta o resultado\n",
    "    check = spark.sql(query_check).collect()[0]\n",
    "    \n",
    "    erros = []\n",
    "    \n",
    "    # 1. Validação de Anos (2024/2025 apenas)\n",
    "    if check['erro_anos'] > 0:\n",
    "        erros.append(f\"Regra de Ano: {check['erro_anos']} registros fora de 2024/2025.\")\n",
    "        \n",
    "    # 2. Validação de Negativos\n",
    "    if check['erro_negativos'] > 0:\n",
    "        erros.append(f\"Regra de Negativos: {check['erro_negativos']} registros com casos negativos.\")\n",
    "        \n",
    "    # 3. Validação de Nulos\n",
    "    if check['erro_nulos'] > 0:\n",
    "        erros.append(f\"Regra de Nulos: {check['erro_nulos']} registros com data vazia.\")\n",
    "        \n",
    "    # Verifica se houve algum erro\n",
    "    if erros:\n",
    "        msg_erro = \"\\n\".join(erros)\n",
    "        log(f\"FALHA CRÍTICA DE DATA QUALITY:\\n{msg_erro}\", \"ERROR\")\n",
    "        raise ValueError(f\"\uD83D\uDEA8 O notebook foi interrompido para segurança dos dados.\")\n",
    "    \n",
    "    log(f\"Dados Aprovados: {check['total_linhas']} registros validados (2024-2025, Sem Nulos, Sem Negativos).\", \"SUCCESS\")\n",
    "\n",
    "# Roda a validação\n",
    "run_quality_gate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d34703f-4fe1-4a59-9a10-7965c2a23469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query e Teste Unitário\n",
    "\n",
    "@tool\n",
    "def get_latest_srag_metrics() -> str:\n",
    "    \"\"\"\n",
    "    Consulta o banco de dados para obter as métricas mais recentes de SRAG.\n",
    "    Retorna: Um JSON com total de casos, e taxas de mortalidade, UTI e vacinação.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # log(\"Consultando métricas no Lakehouse...\", \"TOOL\") # Opcional: Descomentar se quiser muito detalhe\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cast(data_referencia as string) as data_referencia,\n",
    "            cast(total_casos as int) as total_casos,\n",
    "            cast(taxa_aumento_casos_perc as double) as taxa_aumento_casos_perc,\n",
    "            cast(taxa_mortalidade_perc as double) as taxa_mortalidade_perc,\n",
    "            cast(taxa_ocupacao_uti_perc as double) as taxa_ocupacao_uti_perc,\n",
    "            cast(taxa_vacinacao_pacientes_perc as double) as taxa_vacinacao_pacientes_perc\n",
    "        FROM {TABLE_DAILY}\n",
    "        ORDER BY data_referencia DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query).toPandas()\n",
    "        \n",
    "        if df.empty:\n",
    "            log(\"ALERTA: Tabela vazia ao buscar métricas.\", \"WARN\")\n",
    "            return \"ALERTA: A tabela SQL retornou vazio. Verifique se o pipeline rodou.\"\n",
    "            \n",
    "        result = df.iloc[0].to_dict()\n",
    "\n",
    "        # Enriquecimento semântico \n",
    "        result[\"analysis_scope\"] = \"short_term\"\n",
    "        result[\"comparison_window\"] = \"daily_vs_previous\"\n",
    "        result[\"data_nature\"] = \"preliminary\"\n",
    "\n",
    "        return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na Tool de Métricas: {str(e)}\", \"ERROR\")\n",
    "        return f\"ERRO CRÍTICO NO SPARK/SQL: {str(e)}\"\n",
    "\n",
    "# Teste manual imediato\n",
    "log(\"Teste Unitário - Métricas:\", \"SYSTEM\")\n",
    "print(get_latest_srag_metrics.invoke({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f982fbb-f875-43ca-8115-be05fe6a660e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualização\n",
    "\n",
    "@tool\n",
    "def generate_srag_charts() -> str:\n",
    "    \"\"\"\n",
    "    Gera gráficos de linha (30 dias) e barras (12 meses) sobre SRAG.\n",
    "    Padrão de Data:\n",
    "    - Diário: dd/mm (Ex: 28/12)\n",
    "    - Mensal: mm/aaaa (Ex: 12/2024) - Ano com 4 dígitos para evitar confusão com dia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log(\"Iniciando geração de gráficos...\", \"TOOL\")\n",
    "        \n",
    "        # GRÁFICO 1: DIÁRIO (30 DIAS) - Linha\n",
    "        query_daily = f\"SELECT data_referencia, total_casos FROM {TABLE_DAILY} ORDER BY data_referencia DESC LIMIT 30\"\n",
    "        df_daily = spark.sql(query_daily).toPandas().sort_values('data_referencia')\n",
    "        \n",
    "        path_daily = \"Sem dados diários\"\n",
    "        if not df_daily.empty:\n",
    "            df_daily['data_referencia'] = pd.to_datetime(df_daily['data_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(df_daily['data_referencia'], df_daily['total_casos'], marker='o', color='#1f77b4', linewidth=2)\n",
    "            \n",
    "            # Formatação Eixo X (Diário) -> dd/mm\n",
    "            ax1 = plt.gca()\n",
    "            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator(interval=2)) \n",
    "            \n",
    "            plt.title('Casos SRAG - Últimos 30 Dias')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_daily = f\"{VOLUME_PATH}/grafico_diario.png\"\n",
    "            plt.savefig(path_daily)\n",
    "            plt.close()\n",
    "\n",
    "        # GRÁFICO 2: MENSAL (12 MESES) - Barras\n",
    "        query_monthly = f\"SELECT mes_referencia, total_casos FROM {TABLE_MONTHLY} ORDER BY mes_referencia DESC LIMIT 12\"\n",
    "        df_monthly = spark.sql(query_monthly).toPandas().sort_values('mes_referencia')\n",
    "        \n",
    "        path_monthly = \"Sem dados mensais\"\n",
    "        if not df_monthly.empty:\n",
    "            df_monthly['mes_referencia'] = pd.to_datetime(df_monthly['mes_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(df_monthly['mes_referencia'], df_monthly['total_casos'], color='#ff7f0e', width=20)\n",
    "            \n",
    "            # Formatação Eixo X (Mensal) -> mm/aaaa (Ex: 01/2025)\n",
    "            ax2 = plt.gca()\n",
    "            ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y')) \n",
    "            ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "            \n",
    "            plt.title('Casos SRAG - Últimos 12 Meses')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_monthly = f\"{VOLUME_PATH}/grafico_mensal.png\"\n",
    "            plt.savefig(path_monthly)\n",
    "            plt.close()\n",
    "\n",
    "        log(f\"Gráficos persistidos no Volume: {path_daily}, {path_monthly}\", \"SUCCESS\")\n",
    "        return f\"Gráficos atualizados salvos em:\\n1. {path_daily}\\n2. {path_monthly}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Falha ao gerar gráficos: {e}\", \"ERROR\")\n",
    "        return f\"ERRO AO GERAR GRÁFICOS: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2529545-d1f7-418c-afb3-88c2144298b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contexto dados\n",
    "\n",
    "try:\n",
    "    token = dbutils.secrets.get(scope=\"my_srag_scope\", key=\"tavily_api_key\")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = token\n",
    "    log(\"API Key do Tavily carregada via Secrets.\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao carregar a API Key: {e}\", \"ERROR\")\n",
    "\n",
    "@tool\n",
    "def get_epidemiological_context() -> str:\n",
    "    \"\"\"\n",
    "    Realiza 'Grounding' (Ancoragem) do modelo em dados externos em tempo real.\n",
    "    Estratégia:\n",
    "    1. Busca Macro (Global/OMS) para identificar novas variantes.\n",
    "    2. Busca Micro (Brasil/Fiocruz) para dados epidemiológicos locais.\n",
    "    Isso reduz a chance do modelo inventar contextos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        \n",
    "        # Passo 1: Contexto Internacional\n",
    "        log(\"Investigando cenário Global (OMS/CDC)...\", \"TOOL\")\n",
    "        response_global = client.search(\n",
    "            query=\"Global respiratory virus trends WHO CDC influenza covid epidemiological update\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=2, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Passo 2: Contexto Nacional\n",
    "        log(\"Investigando cenário Brasil (InfoGripe/Fiocruz)...\", \"TOOL\")\n",
    "        response_br = client.search(\n",
    "            query=\"Boletim InfoGripe Fiocruz Brasil cenário atual SRAG covid influenza\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=3, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Montagem do Contexto\n",
    "        contexto = f\"\"\"\n",
    "        | RELATÓRIO DE INTELIGÊNCIA EXTERNA |\n",
    "        1. GLOBAL: {response_global.get('answer', 'N/A')}\n",
    "        2. NACIONAL: {response_br.get('answer', 'N/A')}\n",
    "        \n",
    "        FONTES NACIONAIS:\n",
    "        \"\"\"\n",
    "        for res in response_br.get('results', []):\n",
    "            contexto += f\"- {res['title']}: {res['content'][:200]}...\\n\"\n",
    "            \n",
    "        return contexto\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na busca externa: {str(e)}\", \"ERROR\")\n",
    "        return f\"Erro na busca externa: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2119276c-3f38-443d-8d0f-51732c12170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição do Agente\n",
    "\n",
    "# 1. Configuração do Modelo: O parâmetro 'model' define qual endpoint de inferência será acionado.\n",
    "llm = ChatDatabricks(model=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "# 2. Binding das Ferramentas\n",
    "# O Agente precisa de uma lista de 'tools' disponíveis para saber o que ele pode fazer.\n",
    "tools = [get_latest_srag_metrics, generate_srag_charts, get_epidemiological_context]\n",
    "\n",
    "# 3. Criação do Executor (runtime que pega o pensamento do LLM e efetivamente roda as ferramentas Python).\n",
    "try:\n",
    "    agent_executor = create_agent(llm, tools)\n",
    "    log(f\"Agente configurado e pronto! Modelo: {llm.model}\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao criar agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69fc1f5-0112-443e-a7b0-40fe031fa9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-07 20:32:05] [SYSTEM] ⚙️ Iniciando ciclo de análise do Agente...\n[2026-01-07 20:32:06] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: get_latest_srag_metrics\n[2026-01-07 20:32:06] [TOOL] \uD83D\uDEE0️ Iniciando geração de gráficos...\n[2026-01-07 20:32:08] [SUCCESS] ✅ Gráficos persistidos no Volume: /Volumes/srag_prod/gold/volume_imagens/grafico_diario.png, /Volumes/srag_prod/gold/volume_imagens/grafico_mensal.png\n[2026-01-07 20:32:08] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: generate_srag_charts\n[2026-01-07 20:32:08] [TOOL] \uD83D\uDEE0️ Investigando cenário Global (OMS/CDC)...\n[2026-01-07 20:32:08] [TOOL] \uD83D\uDEE0️ Investigando cenário Brasil (InfoGripe/Fiocruz)...\n[2026-01-07 20:32:09] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: get_epidemiological_context\n\n--- [AGENTE PENSANDO / RESPOSTA] ---\n## \uD83D\uDCCA Análise dos Dados Internos (DataSUS)\n- Total de casos: 4\n- Taxa de aumento de casos: Redução de 33.33%\n- Óbitos: Não houve registro de óbitos no período analisado.\n- UTI: 25.0%\n- Vacinação: 25.0%\n- Nota: Devido à natureza preliminar dos dados e possíveis atrasos na notificação, esses números devem ser interpretados com cautela.\n\n## \uD83C\uDF0D Panorama Global\n- Houve um aumento significativo na atividade de vírus respiratórios nos Estados Unidos, com destaque para a Rhode Island e Minnesota, onde foram registrados aumentos nos casos de flu e COVID-19, além de mortes relacionadas a essas condições.\n- Um novo subclado de influenza, conhecido como \"super flu\" ou Subclade K, foi identificado, mas até o momento não se mostrou causar doenças mais severas.\n\n## \uD83C\uDDE7\uD83C\uDDF7 Cenário Brasil (InfoGripe/Fiocruz)\n- De acordo com o Boletim InfoGripe Fiocruz, 32 estados e jurisdições brasileiros apresentam níveis altos ou muito altos de atividade de influenza.\n- A nova cepa de influenza, Subclade K ou \"super flu\", também foi detectada no Brasil, mas sem indícios de causar doenças mais graves.\n\n## \uD83D\uDE80 Conclusão Técnica\n- Sugere-se monitoramento contínuo da situação epidemiológica, Considerando o aumento da atividade viral e a identificação de novas cepas, como o \"super flu\".\n- É importante manter a vigilância sobre os sintomas gripais e buscar atendimento médico imediato em caso de sintomas severos, como dificuldade para respirar, dor no peito persistente ou febre alta.\n\n==================================================\n[2026-01-07 20:32:14] [SUCCESS] ✅ Processo finalizado às 20:32:14\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-03097e9d2dce046f4f548a36ea235f6e\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-03097e9d2dce046f4f548a36ea235f6e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execução final\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# 1. Configuração de Data/Hora\n",
    "fuso_br = timezone(timedelta(hours=-3))\n",
    "data_hora = datetime.now(fuso_br).strftime(\"%d/%m/%Y às %H:%M\")\n",
    "\n",
    "# 2. Definição do System Prompt\n",
    "system_instructions = \"\"\"\n",
    "ATENÇÃO: Atue como Analista de SRAG. Siga ESTRITAMENTE a ordem: \n",
    "1. Execute `get_latest_srag_metrics` e `generate_srag_charts`.\n",
    "2. Execute `get_epidemiological_context` (aguarde retorno).\n",
    "3. Gere o relatório seguindo as regras abaixo.\n",
    "\n",
    "### \uD83D\uDEAB GUARDRAILS (REGRAS DE OURO)\n",
    "1. MORTALIDADE ZERO:\n",
    "   - SE `óbitos` ou `taxa_mortalidade` for 0 ou 0%:\n",
    "     ESCREVA APENAS: \"Não houve registro de óbitos no período analisado.\"\n",
    "   - É PROIBIDO escrever \"taxa de 0%\" ou \"mortalidade nula\".\n",
    "\n",
    "2. VALORES NEGATIVOS (VARIAÇÃO):\n",
    "   - SE `taxa_aumento` for negativa (ex: -10%):\n",
    "     ESCREVA: \"Redução de 10%\" ou \"Queda de 10%\".\n",
    "   - É PROIBIDO escrever \"aumento negativo\" ou \"aumento de -X%\".\n",
    "\n",
    "3. SEMÂNTICA TEMPORAL:\n",
    "   - Para dados recentes (curto prazo), NUNCA use a palavra \"tendência\".\n",
    "   - Use: \"variação pontual\", \"sinal recente\" ou \"dados preliminares\".\n",
    "\n",
    "### \uD83D\uDCDD ESTRUTURA DO RELATÓRIO\n",
    "\n",
    "## \uD83D\uDCCA Análise dos Dados Internos (DataSUS)\n",
    "- Apresente: Total de casos, UTI, Vacinação e Óbitos.\n",
    "- Aplique o GUARDRAIL 2 para variações percentuais.\n",
    "- Aplique o GUARDRAIL 1 para óbitos.\n",
    "- Adicione uma única nota sobre cautela/atraso de notificação no final.\n",
    "\n",
    "## \uD83C\uDF0D Panorama Global\n",
    "- Resuma estritamente o texto retornado pela ferramenta externa. Sem inferências.\n",
    "\n",
    "## \uD83C\uDDE7\uD83C\uDDF7 Cenário Brasil (InfoGripe/Fiocruz)\n",
    "- Resuma estritamente o retorno da ferramenta externa. Se vazio, informe \"Sem dados retornados\".\n",
    "\n",
    "## \uD83D\uDE80 Conclusão Técnica\n",
    "- Texto curto e condicional (ex: \"Sugere-se monitoramento...\"). Evite afirmações absolutas sobre o futuro.\n",
    "\"\"\"\n",
    "\n",
    "log(f\"Iniciando ciclo de análise do Agente...\", \"SYSTEM\")\n",
    "\n",
    "# 3. Montagem do Payload\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=system_instructions),\n",
    "        # Aqui entra o PROMPT DO USUÁRIO\n",
    "        (\"user\", \"Gere o relatório de monitoramento SRAG de hoje.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Execução com Logs Detalhados\n",
    "try:\n",
    "    for chunk in agent_executor.stream(inputs, stream_mode=\"values\", config={\"recursion_limit\": 25}):\n",
    "        message = chunk[\"messages\"][-1]\n",
    "        \n",
    "        if message.type == \"ai\" and not message.tool_calls:\n",
    "            # Mostra o pensamento ou a resposta final\n",
    "            print(f\"\\n--- [AGENTE PENSANDO / RESPOSTA] ---\\n{message.content}\")\n",
    "        elif message.type == \"tool\":\n",
    "            log(f\"Ferramenta acionada: {message.name}\", \"TOOL\")\n",
    "    \n",
    "    hora_fim = datetime.now(fuso_br).strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\" + (\"=\" * 50))\n",
    "    log(f\"Processo finalizado às {hora_fim}\", \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Erro na execução do loop do agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0633c2ef-42c1-41d4-97bc-e2044dd49461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registro de Execução\n",
    "\n",
    "# Salvar o resultado do agente\n",
    "with mlflow.start_run(run_name=\"Relatorio Diario SRAG\") as run:\n",
    "    log(\"Iniciando registro de auditoria no MLflow...\", \"INFO\")\n",
    "    \n",
    "    # 1. Logamos os Parâmetros\n",
    "    mlflow.log_param(\"data_execucao\", data_hora)\n",
    "    mlflow.log_param(\"modelo_usado\", \"llama-3-70b\")\n",
    "    \n",
    "    # 2. Log do Texto Final\n",
    "    nome_arquivo_relatorio = \"relatorio_srag.md\"\n",
    "    \n",
    "    # Pega a última mensagem válida do loop anterior\n",
    "    # (Nota: Assume que a variável 'message' ainda está na memória da célula anterior)\n",
    "    try:\n",
    "        texto_final = message.content \n",
    "        mlflow.log_text(texto_final, nome_arquivo_relatorio)\n",
    "        log(f\"Texto do relatório salvo: {nome_arquivo_relatorio}\", \"INFO\")\n",
    "    except NameError:\n",
    "        log(\"Variável 'message' não encontrada. O agente rodou?\", \"WARN\")\n",
    "    \n",
    "    # 3. Logamos os Gráficos. O MLflow copia as imagens do Volume para dentro do Experimento\n",
    "    try:\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_diario.png\", artifact_path=\"graficos\")\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_mensal.png\", artifact_path=\"graficos\")\n",
    "        log(\"Artefatos visuais (Gráficos) anexados ao experimento.\", \"SUCCESS\")\n",
    "    except Exception as e:\n",
    "        log(f\"Não foi possível logar as imagens: {e}\", \"WARN\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    log(\"Sucesso! Execução auditada no MLflow.\", \"SUCCESS\")\n",
    "    log(f\"Link para Experiment: {experiment_path}\", \"INFO\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "srag_agent_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}