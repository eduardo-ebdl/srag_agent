{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef39dfe-0f9e-4b3b-af1b-4473f27a1162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n  Downloading langchain-1.2.3-py3-none-any.whl.metadata (4.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-core\n  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph\n  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\nCollecting databricks-langchain\n  Downloading databricks_langchain-0.12.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting tavily-python\n  Downloading tavily_python-0.7.17-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.12/site-packages (3.10.0)\nCollecting matplotlib\n  Downloading matplotlib-3.10.8-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (52 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (2.2.3)\nCollecting pandas\n  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\nCollecting mlflow\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.10.6)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (9.5 kB)\nCollecting requests<3.0.0,>=2.32.5 (from langchain-community)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n  Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (8.1 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\nCollecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.6.2-py3-none-any.whl.metadata (15 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: numpy>=1.26.2 in /databricks/python3/lib/python3.12/site-packages (from langchain-community) (2.1.3)\nCollecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core) (24.1)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core) (4.12.2)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core)\n  Downloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_aarch64.whl.metadata (5.4 kB)\nCollecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\nCollecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n  Downloading langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting xxhash>=3.5.0 (from langgraph)\n  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting databricks-ai-bridge>=0.4.2 (from databricks-langchain)\n  Downloading databricks_ai_bridge-0.11.0-py3-none-any.whl.metadata (6.9 kB)\nCollecting databricks-mcp>=0.5.1 (from databricks-langchain)\n  Downloading databricks_mcp-0.5.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting databricks-sdk>=0.65.0 (from databricks-langchain)\n  Downloading databricks_sdk-0.77.0-py3-none-any.whl.metadata (40 kB)\nCollecting databricks-vectorsearch>=0.50 (from databricks-langchain)\n  Downloading databricks_vectorsearch-0.63-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain-mcp-adapters>=0.1.13 (from databricks-langchain)\n  Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl.metadata (10 kB)\nCollecting openai>=1.99.9 (from databricks-langchain)\n  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\nCollecting unitycatalog-langchain>=0.3.0 (from unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_langchain-0.3.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting tiktoken>=0.5.1 (from tavily-python)\n  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.7 kB)\nRequirement already satisfied: httpx in /databricks/python3/lib/python3.12/site-packages (from tavily-python) (0.27.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=3 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nCollecting mlflow-skinny==3.8.1 (from mlflow)\n  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.8.1 (from mlflow)\n  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\nCollecting Flask-CORS<7 (from mlflow)\n  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\nCollecting Flask<4 (from mlflow)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow)\n  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (43.0.3)\nCollecting docker<8,>=4.0.0 (from mlflow)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting huey<3,>=2.5.0 (from mlflow)\n  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.15.1)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.29.4)\nCollecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.3)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.34.2)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (75 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow) (1.17.1)\nCollecting tabulate>=0.9.0 (from databricks-ai-bridge>=0.4.2->databricks-langchain)\n  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting mcp>=1.9.1 (from databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading mcp-1.25.0-py3-none-any.whl.metadata (89 kB)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.65.0->databricks-langchain) (2.40.0)\nCollecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_aarch64.whl.metadata (593 bytes)\nCollecting deprecation>=2 (from databricks-vectorsearch>=0.50->databricks-langchain)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: jinja2>=3.1.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.5)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.2)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\nCollecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\nCollecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core)\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.2 kB)\nCollecting orjson>=3.10.1 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (41 kB)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx->tavily-python) (4.6.2)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx->tavily-python) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx->tavily-python) (3.7)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\nCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.9->databricks-langchain) (1.9.0)\nCollecting jiter<1,>=0.10.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.2 kB)\nCollecting tqdm>4 (from openai>=1.99.9->databricks-langchain)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\nCollecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nCollecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n  Downloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (4.1 kB)\nCollecting regex>=2022.1.18 (from tiktoken>=0.5.1->tavily-python)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nCollecting unitycatalog-ai (from unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_ai-0.3.2-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.21)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.11)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (4.9.1)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.21.0)\nCollecting httpx (from tavily-python)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: jsonschema>=4.20.0 in /databricks/python3/lib/python3.12/site-packages (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (4.23.0)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain)\n  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\nRequirement already satisfied: pyjwt>=2.10.1 in /databricks/python3/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (2.10.1)\nCollecting python-multipart>=0.0.9 (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading sse_starlette-3.1.2-py3-none-any.whl.metadata (12 kB)\nCollecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.3 kB)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.53b1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.12/site-packages (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.6.0)\nCollecting unitycatalog-client (from unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading unitycatalog_client-0.3.1-py3-none-any.whl.metadata (7.8 kB)\nCollecting databricks-connect<17.1,>=15.1.0 (from unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading databricks_connect-17.0.10-py2.py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: grpcio>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (1.67.0)\nRequirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (0.10.9.9)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect<17.1,>=15.1.0->unitycatalog-ai[databricks]; extra == \"databricks\"->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain) (74.0.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain) (0.22.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.65.0->databricks-langchain) (0.4.8)\nINFO: pip is looking at multiple versions of sse-starlette to determine which version is compatible with other requirements. This could take a while.\nCollecting sse-starlette>=1.6.1 (from mcp>=1.9.1->databricks-mcp>=0.5.1->databricks-langchain)\n  Downloading sse_starlette-3.1.1-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.1.0-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.0.4-py3-none-any.whl.metadata (12 kB)\n  Downloading sse_starlette-3.0.3-py3-none-any.whl.metadata (12 kB)\nCollecting anyio<5,>=3.5.0 (from openai>=1.99.9->databricks-langchain)\n  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting aiohttp-retry>=2.8.3 (from unitycatalog-client->unitycatalog-ai->unitycatalog-langchain>=0.3.0->unitycatalog-langchain[databricks]>=0.3.0->databricks-langchain)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nCollecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_aarch64.whl.metadata (592 bytes)\nDownloading langchain-1.2.3-py3-none-any.whl (106 kB)\nDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m58.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\nDownloading langgraph-1.0.5-py3-none-any.whl (157 kB)\nDownloading databricks_langchain-0.12.1-py3-none-any.whl (35 kB)\nDownloading tavily_python-0.7.17-py3-none-any.whl (18 kB)\nDownloading matplotlib-3.10.8-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (9.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m156.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (11.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/11.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.7/11.7 MB\u001B[0m \u001B[31m136.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.1/9.1 MB\u001B[0m \u001B[31m108.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m75.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m57.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohttp-3.13.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m62.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading alembic-1.17.2-py3-none-any.whl (248 kB)\nDownloading databricks_ai_bridge-0.11.0-py3-none-any.whl (23 kB)\nDownloading databricks_mcp-0.5.1-py3-none-any.whl (11 kB)\nDownloading databricks_sdk-0.77.0-py3-none-any.whl (779 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/779.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m779.2/779.2 kB\u001B[0m \u001B[31m38.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.63-py3-none-any.whl (19 kB)\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\nDownloading huey-2.6.0-py3-none-any.whl (76 kB)\nDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_mcp_adapters-0.2.1-py3-none-any.whl (22 kB)\nDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\nDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\nDownloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\nDownloading langsmith-0.6.2-py3-none-any.whl (282 kB)\nDownloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m58.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\nDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m83.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m61.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nDownloading unitycatalog_langchain-0.3.0-py3-none-any.whl (5.3 kB)\nDownloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_aarch64.whl (340 kB)\nDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (212 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading frozenlist-1.8.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (243 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.3.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (597 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/597.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m597.3/597.3 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (350 kB)\nDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\nDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\nDownloading mcp-1.25.0-py3-none-any.whl (233 kB)\nDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\nDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\nDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m67.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (258 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\nDownloading orjson-3.11.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (132 kB)\nDownloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (202 kB)\nDownloading propcache-0.4.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (225 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading regex-2025.11.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (798 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/798.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.6/798.6 kB\u001B[0m \u001B[31m36.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\nDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading yarl-1.22.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (372 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nDownloading unitycatalog_ai-0.3.2-py3-none-any.whl (66 kB)\nDownloading databricks_connect-17.0.10-py2.py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m92.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading python_multipart-0.0.21-py3-none-any.whl (24 kB)\nDownloading sse_starlette-3.0.3-py3-none-any.whl (11 kB)\nDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\nDownloading unitycatalog_client-0.3.1-py3-none-any.whl (176 kB)\nDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_aarch64.whl (319 kB)\nInstalling collected packages: huey, xxhash, werkzeug, uuid-utils, typing-extensions, tqdm, tabulate, requests, regex, python-multipart, python-dotenv, protobuf, propcache, ormsgpack, orjson, multidict, marshmallow, Mako, jsonpatch, jiter, itsdangerous, httpx-sse, gunicorn, greenlet, graphql-core, frozenlist, deprecation, blinker, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, pydantic-core, pandas, opentelemetry-proto, matplotlib, graphql-relay, Flask, docker, anyio, aiosignal, sse-starlette, pydantic, httpx, graphene, Flask-CORS, dataclasses-json, databricks-sdk, alembic, aiohttp, tavily-python, pydantic-settings, openai, langsmith, langgraph-sdk, databricks-connect, aiohttp-retry, unitycatalog-client, mlflow-tracing, mlflow-skinny, mcp, langchain-core, unitycatalog-ai, mlflow, langgraph-checkpoint, langchain-text-splitters, langchain-mcp-adapters, databricks-vectorsearch, langgraph-prebuilt, langchain-classic, databricks-ai-bridge, langgraph, langchain-community, databricks-mcp, langchain, unitycatalog-langchain, databricks-langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.27.2\n    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Not uninstalling pandas at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'pandas'. No files were found to uninstall.\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.10.0\n    Not uninstalling matplotlib at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'matplotlib'. No files were found to uninstall.\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.6.2\n    Not uninstalling anyio at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'anyio'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Not uninstalling httpx at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'httpx'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: databricks-connect\n    Found existing installation: databricks-connect 17.2.4\n    Not uninstalling databricks-connect at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'databricks-connect'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.0\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5d517178-590b-432b-975c-1be9061f412e\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiohttp-retry-2.9.1 aiosignal-1.4.0 alembic-1.17.2 anyio-4.12.1 blinker-1.9.0 databricks-ai-bridge-0.11.0 databricks-connect-17.0.10 databricks-langchain-0.12.1 databricks-mcp-0.5.1 databricks-sdk-0.77.0 databricks-vectorsearch-0.63 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 frozenlist-1.8.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.0 gunicorn-23.0.0 httpx-0.28.1 httpx-sse-0.4.3 huey-2.6.0 itsdangerous-2.2.0 jiter-0.12.0 jsonpatch-1.33 langchain-1.2.3 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.6 langchain-mcp-adapters-0.2.1 langchain-text-splitters-1.1.0 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.6.2 marshmallow-3.26.2 matplotlib-3.10.8 mcp-1.25.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 multidict-6.7.0 openai-2.14.0 opentelemetry-proto-1.39.1 orjson-3.11.5 ormsgpack-1.12.1 pandas-2.3.3 propcache-0.4.1 protobuf-5.29.5 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 python-multipart-0.0.21 regex-2025.11.3 requests-2.32.5 requests-toolbelt-1.0.0 sse-starlette-3.0.3 tabulate-0.9.0 tavily-python-0.7.17 tiktoken-0.12.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 unitycatalog-ai-0.3.2 unitycatalog-client-0.3.1 unitycatalog-langchain-0.3.0 uuid-utils-0.13.0 werkzeug-3.1.5 xxhash-3.6.0 yarl-1.22.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalação das bibliotecas de Orquestração (LangChain/Graph), Conectores (Databricks) e Pesquisa (Tavily).\n",
    "%pip install -U langchain langchain-community langchain-core langgraph databricks-langchain tavily-python matplotlib pandas mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70594b0-22fb-4dc1-b2f9-89f02c6d02b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:44:55] [SYSTEM] ⚙️ Bibliotecas importadas e Logger configurado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Importações\n",
    "\n",
    "# Bibliotecas padrão\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Dados e visualização\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# LangChain e LLMs\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Serviços externos e ML\n",
    "from tavily import TavilyClient\n",
    "import mlflow\n",
    "\n",
    "# Configuração de Log\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"\n",
    "    Logger padronizado para rastreabilidade de execução.\n",
    "    Formato: [YYYY-MM-DD HH:MM:SS] [LEVEL] Icon Mensagem\n",
    "    \"\"\"\n",
    "    # Configuração de Fuso Horário (BRT)\n",
    "    fuso_br = timezone(timedelta(hours=-3))\n",
    "    timestamp = datetime.now(fuso_br).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Ícones visuais para facilitar leitura rápida dos logs\n",
    "    icons = {\n",
    "        \"INFO\": \"ℹ️\", \n",
    "        \"WARN\": \"⚠️\", \n",
    "        \"ERROR\": \"❌\", \n",
    "        \"SUCCESS\": \"✅\", \n",
    "        \"SYSTEM\": \"⚙️\",\n",
    "        \"TOOL\": \"\uD83D\uDEE0️\",\n",
    "        \"AI\": \"\uD83E\uDD16\"\n",
    "    }\n",
    "    icon = icons.get(level, \"\")\n",
    "    \n",
    "    print(f\"[{timestamp}] [{level}] {icon} {msg}\")\n",
    "\n",
    "log(\"Bibliotecas importadas e Logger configurado com sucesso.\", \"SYSTEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf0b0c2-cbad-4f5a-8726-93052e2b8351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:44:58] [SUCCESS] ✅ MLOps Ativado. Experiment Path: /Users/eduardobdel@gmail.com/srag_agent_monitoring\n"
     ]
    }
   ],
   "source": [
    "# Define o experimento\n",
    "\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/srag_agent_monitoring\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "# Habilita o rastreamento automático para LangChain (capturar inputs, outputs, traces)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "log(f\"MLOps Ativado. Experiment Path: {experiment_path}\", \"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c79805a-fffb-4e07-b9af-d7b20d7ca16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:44:59] [SYSTEM] ⚙️ Ambiente de Dados Configurado.\n[2026-01-09 09:44:59] [INFO] ℹ️ Caminho do Volume: /Volumes/srag_prod/gold/volume_imagens\n[2026-01-09 09:44:59] [INFO] ℹ️ Tabelas Mapeadas: srag_prod.gold.gold_srag_daily, srag_prod.gold.gold_srag_monthly\n"
     ]
    }
   ],
   "source": [
    "# Arquitetura dos dados\n",
    "\n",
    "CATALOG = \"srag_prod\" \n",
    "SCHEMA = \"gold\"       \n",
    "VOLUME_NAME = \"volume_imagens\" # Storage para arquivos não estruturados (png)\n",
    "\n",
    "# Garante a existência do volume para persistência dos gráficos gerados pelo agente\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME_NAME}\")\n",
    "\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "TABLE_DAILY = f\"{CATALOG}.{SCHEMA}.gold_srag_daily\"\n",
    "TABLE_MONTHLY = f\"{CATALOG}.{SCHEMA}.gold_srag_monthly\"\n",
    "\n",
    "log(f\"Ambiente de Dados Configurado.\", \"SYSTEM\")\n",
    "log(f\"Caminho do Volume: {VOLUME_PATH}\", \"INFO\")\n",
    "log(f\"Tabelas Mapeadas: {TABLE_DAILY}, {TABLE_MONTHLY}\", \"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba5a11f-8e3e-4b63-9541-91e3e6397da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:00] [SYSTEM] ⚙️ Executando Validação de Qualidade de Dados (Data Quality Gate)...\n[2026-01-09 09:45:05] [SUCCESS] ✅ Dados Aprovados: 721 registros validados (2024-2025, Sem Nulos, Sem Negativos).\n"
     ]
    }
   ],
   "source": [
    "# Validação dos dados\n",
    "\n",
    "def run_quality_gate():\n",
    "    \"\"\"\n",
    "    Valida os dados da tabela Gold. Se falhar, interrompe o notebook.\n",
    "    Regras:\n",
    "    1. Apenas anos 2024 e 2025.\n",
    "    2. Sem casos negativos.\n",
    "    3. Sem datas nulas.\n",
    "    \"\"\"\n",
    "    log(\"Executando Validação de Qualidade de Dados (Data Quality Gate)...\", \"SYSTEM\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # A função 'sum' conta quantas linhas violam cada regra\n",
    "    query_check = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_linhas,\n",
    "            SUM(CASE WHEN year(data_referencia) NOT IN (2024, 2025) THEN 1 ELSE 0 END) as erro_anos,\n",
    "            SUM(CASE WHEN total_casos < 0 THEN 1 ELSE 0 END) as erro_negativos,\n",
    "            SUM(CASE WHEN data_referencia IS NULL THEN 1 ELSE 0 END) as erro_nulos\n",
    "        FROM {TABLE_DAILY}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coleta o resultado\n",
    "    check = spark.sql(query_check).collect()[0]\n",
    "    \n",
    "    erros = []\n",
    "    \n",
    "    # 1. Validação de Anos (2024/2025 apenas)\n",
    "    if check['erro_anos'] > 0:\n",
    "        erros.append(f\"Regra de Ano: {check['erro_anos']} registros fora de 2024/2025.\")\n",
    "        \n",
    "    # 2. Validação de Negativos\n",
    "    if check['erro_negativos'] > 0:\n",
    "        erros.append(f\"Regra de Negativos: {check['erro_negativos']} registros com casos negativos.\")\n",
    "        \n",
    "    # 3. Validação de Nulos\n",
    "    if check['erro_nulos'] > 0:\n",
    "        erros.append(f\"Regra de Nulos: {check['erro_nulos']} registros com data vazia.\")\n",
    "        \n",
    "    # Verifica se houve algum erro\n",
    "    if erros:\n",
    "        msg_erro = \"\\n\".join(erros)\n",
    "        log(f\"FALHA CRÍTICA DE DATA QUALITY:\\n{msg_erro}\", \"ERROR\")\n",
    "        raise ValueError(f\"\uD83D\uDEA8 O notebook foi interrompido para segurança dos dados.\")\n",
    "    \n",
    "    log(f\"Dados Aprovados: {check['total_linhas']} registros validados (2024-2025, Sem Nulos, Sem Negativos).\", \"SUCCESS\")\n",
    "\n",
    "# Roda a validação\n",
    "run_quality_gate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d34703f-4fe1-4a59-9a10-7965c2a23469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:05] [SYSTEM] ⚙️ Teste Unitário - Métricas:\n{\"data_referencia\": \"2025-12-21\", \"total_casos\": 4, \"taxa_aumento_casos_perc\": -33.33, \"taxa_mortalidade_perc\": 0.0, \"taxa_ocupacao_uti_perc\": 25.0, \"taxa_vacinacao_pacientes_perc\": 25.0, \"analysis_scope\": \"short_term\", \"comparison_window\": \"daily_vs_previous\", \"data_nature\": \"preliminary\"}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-3e6b4ee62fcbd4ea15c73d2822dd62ea\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-3e6b4ee62fcbd4ea15c73d2822dd62ea)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query e Teste Unitário\n",
    "\n",
    "@tool\n",
    "def get_latest_srag_metrics() -> str:\n",
    "    \"\"\"\n",
    "    Consulta o banco de dados para obter as métricas mais recentes de SRAG.\n",
    "    Retorna: Um JSON com total de casos, e taxas de mortalidade, UTI e vacinação.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # log(\"Consultando métricas no Lakehouse...\", \"TOOL\") # Opcional: Descomentar se quiser muito detalhe\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cast(data_referencia as string) as data_referencia,\n",
    "            cast(total_casos as int) as total_casos,\n",
    "            cast(taxa_aumento_casos_perc as double) as taxa_aumento_casos_perc,\n",
    "            cast(taxa_mortalidade_perc as double) as taxa_mortalidade_perc,\n",
    "            cast(taxa_ocupacao_uti_perc as double) as taxa_ocupacao_uti_perc,\n",
    "            cast(taxa_vacinacao_pacientes_perc as double) as taxa_vacinacao_pacientes_perc\n",
    "        FROM {TABLE_DAILY}\n",
    "        ORDER BY data_referencia DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query).toPandas()\n",
    "        \n",
    "        if df.empty:\n",
    "            log(\"ALERTA: Tabela vazia ao buscar métricas.\", \"WARN\")\n",
    "            return \"ALERTA: A tabela SQL retornou vazio. Verifique se o pipeline rodou.\"\n",
    "            \n",
    "        result = df.iloc[0].to_dict()\n",
    "\n",
    "        # Enriquecimento semântico \n",
    "        result[\"analysis_scope\"] = \"short_term\"\n",
    "        result[\"comparison_window\"] = \"daily_vs_previous\"\n",
    "        result[\"data_nature\"] = \"preliminary\"\n",
    "\n",
    "        return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na Tool de Métricas: {str(e)}\", \"ERROR\")\n",
    "        return f\"ERRO CRÍTICO NO SPARK/SQL: {str(e)}\"\n",
    "\n",
    "# Teste manual imediato\n",
    "log(\"Teste Unitário - Métricas:\", \"SYSTEM\")\n",
    "print(get_latest_srag_metrics.invoke({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f982fbb-f875-43ca-8115-be05fe6a660e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualização\n",
    "\n",
    "@tool\n",
    "def generate_srag_charts() -> str:\n",
    "    \"\"\"\n",
    "    Gera gráficos de linha (30 dias) e barras (12 meses) sobre SRAG.\n",
    "    Padrão de Data:\n",
    "    - Diário: dd/mm (Ex: 28/12)\n",
    "    - Mensal: mm/aaaa (Ex: 12/2024) - Ano com 4 dígitos para evitar confusão com dia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log(\"Iniciando geração de gráficos...\", \"TOOL\")\n",
    "        \n",
    "        # GRÁFICO 1: DIÁRIO (30 DIAS) - Linha\n",
    "        query_daily = f\"SELECT data_referencia, total_casos FROM {TABLE_DAILY} ORDER BY data_referencia DESC LIMIT 30\"\n",
    "        df_daily = spark.sql(query_daily).toPandas().sort_values('data_referencia')\n",
    "        \n",
    "        path_daily = \"Sem dados diários\"\n",
    "        if not df_daily.empty:\n",
    "            df_daily['data_referencia'] = pd.to_datetime(df_daily['data_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(df_daily['data_referencia'], df_daily['total_casos'], marker='o', color='#1f77b4', linewidth=2)\n",
    "            \n",
    "            # Formatação Eixo X (Diário) -> dd/mm\n",
    "            ax1 = plt.gca()\n",
    "            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator(interval=2)) \n",
    "            \n",
    "            plt.title('Casos SRAG - Últimos 30 Dias')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_daily = f\"{VOLUME_PATH}/grafico_diario.png\"\n",
    "            plt.savefig(path_daily)\n",
    "            plt.close()\n",
    "\n",
    "        # GRÁFICO 2: MENSAL (12 MESES) - Barras\n",
    "        query_monthly = f\"SELECT mes_referencia, total_casos FROM {TABLE_MONTHLY} ORDER BY mes_referencia DESC LIMIT 12\"\n",
    "        df_monthly = spark.sql(query_monthly).toPandas().sort_values('mes_referencia')\n",
    "        \n",
    "        path_monthly = \"Sem dados mensais\"\n",
    "        if not df_monthly.empty:\n",
    "            df_monthly['mes_referencia'] = pd.to_datetime(df_monthly['mes_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(df_monthly['mes_referencia'], df_monthly['total_casos'], color='#ff7f0e', width=20)\n",
    "            \n",
    "            # Formatação Eixo X (Mensal) -> mm/aaaa (Ex: 01/2025)\n",
    "            ax2 = plt.gca()\n",
    "            ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y')) \n",
    "            ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "            \n",
    "            plt.title('Casos SRAG - Últimos 12 Meses')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_monthly = f\"{VOLUME_PATH}/grafico_mensal.png\"\n",
    "            plt.savefig(path_monthly)\n",
    "            plt.close()\n",
    "\n",
    "        log(f\"Gráficos persistidos no Volume: {path_daily}, {path_monthly}\", \"SUCCESS\")\n",
    "        return f\"Gráficos atualizados salvos em:\\n1. {path_daily}\\n2. {path_monthly}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Falha ao gerar gráficos: {e}\", \"ERROR\")\n",
    "        return f\"ERRO AO GERAR GRÁFICOS: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2529545-d1f7-418c-afb3-88c2144298b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:07] [SUCCESS] ✅ API Key do Tavily carregada via Secrets.\n"
     ]
    }
   ],
   "source": [
    "# Contexto dados\n",
    "\n",
    "try:\n",
    "    token = dbutils.secrets.get(scope=\"my_srag_scope\", key=\"tavily_api_key\")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = token\n",
    "    log(\"API Key do Tavily carregada via Secrets.\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao carregar a API Key: {e}\", \"ERROR\")\n",
    "\n",
    "@tool\n",
    "def get_epidemiological_context() -> str:\n",
    "    \"\"\"\n",
    "    Realiza 'Grounding' (Ancoragem) do modelo em dados externos em tempo real.\n",
    "    Estratégia:\n",
    "    1. Busca Macro (Global/OMS) para identificar novas variantes.\n",
    "    2. Busca Micro (Brasil/Fiocruz) para dados epidemiológicos locais.\n",
    "    Isso reduz a chance do modelo inventar contextos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        \n",
    "        # Passo 1: Contexto Internacional\n",
    "        log(\"Investigando cenário Global (OMS/CDC)...\", \"TOOL\")\n",
    "        response_global = client.search(\n",
    "            query=\"Global respiratory virus trends WHO CDC influenza covid epidemiological update\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=2, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Passo 2: Contexto Nacional\n",
    "        log(\"Investigando cenário Brasil (InfoGripe/Fiocruz)...\", \"TOOL\")\n",
    "        response_br = client.search(\n",
    "            query=\"Boletim InfoGripe Fiocruz Brasil cenário atual SRAG covid influenza\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=3, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Montagem do Contexto\n",
    "        contexto = f\"\"\"\n",
    "        | RELATÓRIO DE INTELIGÊNCIA EXTERNA |\n",
    "        1. GLOBAL: {response_global.get('answer', 'N/A')}\n",
    "        2. NACIONAL: {response_br.get('answer', 'N/A')}\n",
    "        \n",
    "        FONTES NACIONAIS:\n",
    "        \"\"\"\n",
    "        for res in response_br.get('results', []):\n",
    "            contexto += f\"- {res['title']}: {res['content'][:200]}...\\n\"\n",
    "            \n",
    "        return contexto\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na busca externa: {str(e)}\", \"ERROR\")\n",
    "        return f\"Erro na busca externa: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2119276c-3f38-443d-8d0f-51732c12170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:08] [SUCCESS] ✅ Agente configurado e pronto! Modelo: databricks-meta-llama-3-3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Definição do Agente\n",
    "\n",
    "# 1. Configuração do Modelo: O parâmetro 'model' define qual endpoint de inferência será acionado.\n",
    "llm = ChatDatabricks(model=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "# 2. Binding das Ferramentas\n",
    "# O Agente precisa de uma lista de 'tools' disponíveis para saber o que ele pode fazer.\n",
    "tools = [get_latest_srag_metrics, generate_srag_charts, get_epidemiological_context]\n",
    "\n",
    "# 3. Criação do Executor (runtime que pega o pensamento do LLM e efetivamente roda as ferramentas Python).\n",
    "try:\n",
    "    agent_executor = create_agent(llm, tools)\n",
    "    log(f\"Agente configurado e pronto! Modelo: {llm.model}\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao criar agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69fc1f5-0112-443e-a7b0-40fe031fa9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:08] [SYSTEM] ⚙️ Iniciando ciclo de análise do Agente...\n[2026-01-09 09:45:10] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: get_latest_srag_metrics\n[2026-01-09 09:45:10] [TOOL] \uD83D\uDEE0️ Iniciando geração de gráficos...\n[2026-01-09 09:45:13] [SUCCESS] ✅ Gráficos persistidos no Volume: /Volumes/srag_prod/gold/volume_imagens/grafico_diario.png, /Volumes/srag_prod/gold/volume_imagens/grafico_mensal.png\n[2026-01-09 09:45:13] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: generate_srag_charts\n[2026-01-09 09:45:14] [TOOL] \uD83D\uDEE0️ Investigando cenário Global (OMS/CDC)...\n[2026-01-09 09:45:15] [TOOL] \uD83D\uDEE0️ Investigando cenário Brasil (InfoGripe/Fiocruz)...\n[2026-01-09 09:45:16] [TOOL] \uD83D\uDEE0️ Ferramenta acionada: get_epidemiological_context\n\n--- [AGENTE PENSANDO / RESPOSTA] ---\n## \uD83D\uDCCA Análise dos Dados Internos (DataSUS)\n- Total de casos: 4\n- Variação de casos: Redução de 33.33%\n- Taxa de mortalidade: 0.0%\n- Taxa de ocupação em UTI: 25.0%\n- Taxa de vacinação: 25.0%\n- Não houve registro de óbitos no período analisado.\n\nNota: Devido à natureza preliminar dos dados, é importante considerar a possibilidade de atrasos ou alterações nas notificações.\n\n## \uD83C\uDF0D Panorama Global\nDe acordo com o relatório de inteligência externa, há um aumento significativo na atividade de gripe nos Estados Unidos, com 32 estados e jurisdições apresentando níveis \"altos\" ou \"muito altos\" de atividade de gripe. A temporada de gripe está mostrando um número recorde de doenças respiratórias ambulatoriais e um aumento nas hospitalizações.\n\n## \uD83C\uDDE7\uD83C\uDDF7 Cenário Brasil (InfoGripe/Fiocruz)\nO cenário atual para a Síndrome Respiratória Aguda Grave (SRAG) no Brasil indica níveis \"altos\" ou \"muito altos\" de atividade de gripe em 32 estados e jurisdições, de acordo com os dados mais recentes. Um novo subtipo de gripe, conhecido como \"super gripe\", emergiu, mas ainda não há evidências de que cause doenças mais graves em comparação com as cepas anteriores de gripe A.\n\n## \uD83D\uDE80 Conclusão Técnica\nSugere-se monitoramento contínuo e análise dos dados de SRAG, considerando a natureza dinâmica da situação epidemiológica e a possibilidade de novas variantes ou mudanças nos padrões de doença. Além disso, é fundamental manter a vigilância sobre a vacinação e a ocupação de leitos em UTI, visando uma resposta eficaz às necessidades de saúde pública.\n\n==================================================\n[2026-01-09 09:45:22] [SUCCESS] ✅ Processo finalizado às 09:45:22\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "{\"trace_id\": \"tr-adcf33cf6b4f38a93ad4c41f514bad80\", \"sql_warehouse_id\": null}",
      "text/plain": [
       "Trace(trace_id=tr-adcf33cf6b4f38a93ad4c41f514bad80)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execução final\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# 1. Configuração de Data/Hora\n",
    "fuso_br = timezone(timedelta(hours=-3))\n",
    "data_hora = datetime.now(fuso_br).strftime(\"%d/%m/%Y às %H:%M\")\n",
    "\n",
    "# 2. Definição do System Prompt\n",
    "system_instructions = \"\"\"\n",
    "ATENÇÃO: Atue como Analista de SRAG. Siga ESTRITAMENTE a ordem: \n",
    "1. Execute `get_latest_srag_metrics` e `generate_srag_charts`.\n",
    "2. Execute `get_epidemiological_context` (aguarde retorno).\n",
    "3. Gere o relatório seguindo as regras abaixo.\n",
    "\n",
    "### \uD83D\uDEAB GUARDRAILS (REGRAS DE OURO)\n",
    "1. MORTALIDADE ZERO:\n",
    "   - SE `óbitos` ou `taxa_mortalidade` for 0 ou 0%:\n",
    "     ESCREVA APENAS: \"Não houve registro de óbitos no período analisado.\"\n",
    "   - É PROIBIDO escrever \"taxa de 0%\" ou \"mortalidade nula\".\n",
    "\n",
    "2. VALORES NEGATIVOS (VARIAÇÃO):\n",
    "   - SE `taxa_aumento` for negativa (ex: -10%):\n",
    "     ESCREVA: \"Redução de 10%\" ou \"Queda de 10%\".\n",
    "   - É PROIBIDO escrever \"aumento negativo\" ou \"aumento de -X%\".\n",
    "\n",
    "3. SEMÂNTICA TEMPORAL:\n",
    "   - Para dados recentes (curto prazo), NUNCA use a palavra \"tendência\".\n",
    "   - Use: \"variação pontual\", \"sinal recente\" ou \"dados preliminares\".\n",
    "\n",
    "### \uD83D\uDCDD ESTRUTURA DO RELATÓRIO\n",
    "\n",
    "## \uD83D\uDCCA Análise dos Dados Internos (DataSUS)\n",
    "- Apresente: Total de casos, UTI, Vacinação e Óbitos.\n",
    "- Aplique o GUARDRAIL 2 para variações percentuais.\n",
    "- Aplique o GUARDRAIL 1 para óbitos.\n",
    "- Adicione uma única nota sobre cautela/atraso de notificação no final.\n",
    "\n",
    "## \uD83C\uDF0D Panorama Global\n",
    "- Resuma estritamente o texto retornado pela ferramenta externa. Sem inferências.\n",
    "\n",
    "## \uD83C\uDDE7\uD83C\uDDF7 Cenário Brasil (InfoGripe/Fiocruz)\n",
    "- Resuma estritamente o retorno da ferramenta externa. Se vazio, informe \"Sem dados retornados\".\n",
    "\n",
    "## \uD83D\uDE80 Conclusão Técnica\n",
    "- Texto curto e condicional (ex: \"Sugere-se monitoramento...\"). Evite afirmações absolutas sobre o futuro.\n",
    "\"\"\"\n",
    "\n",
    "log(f\"Iniciando ciclo de análise do Agente...\", \"SYSTEM\")\n",
    "\n",
    "# 3. Montagem do Payload\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=system_instructions),\n",
    "        # Aqui entra o PROMPT DO USUÁRIO\n",
    "        (\"user\", \"Gere o relatório de monitoramento SRAG de hoje.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Execução com Logs Detalhados\n",
    "try:\n",
    "    for chunk in agent_executor.stream(inputs, stream_mode=\"values\", config={\"recursion_limit\": 25}):\n",
    "        message = chunk[\"messages\"][-1]\n",
    "        \n",
    "        if message.type == \"ai\" and not message.tool_calls:\n",
    "            # Mostra o pensamento ou a resposta final\n",
    "            print(f\"\\n--- [AGENTE PENSANDO / RESPOSTA] ---\\n{message.content}\")\n",
    "        elif message.type == \"tool\":\n",
    "            log(f\"Ferramenta acionada: {message.name}\", \"TOOL\")\n",
    "    \n",
    "    hora_fim = datetime.now(fuso_br).strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\" + (\"=\" * 50))\n",
    "    log(f\"Processo finalizado às {hora_fim}\", \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Erro na execução do loop do agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0633c2ef-42c1-41d4-97bc-e2044dd49461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-09 09:45:23] [INFO] ℹ️ Iniciando registro de auditoria no MLflow...\n[2026-01-09 09:45:23] [INFO] ℹ️ Texto do relatório salvo: relatorio_srag.md\n[2026-01-09 09:45:24] [SUCCESS] ✅ Artefatos visuais (Gráficos) anexados ao experimento.\n----------------------------------------------------------------------------------------------------\n[2026-01-09 09:45:24] [SUCCESS] ✅ Sucesso! Execução auditada no MLflow.\n[2026-01-09 09:45:24] [INFO] ℹ️ Link para Experiment: /Users/eduardobdel@gmail.com/srag_agent_monitoring\n"
     ]
    }
   ],
   "source": [
    "# Registro de Execução\n",
    "\n",
    "# Salvar o resultado do agente\n",
    "with mlflow.start_run(run_name=\"Relatorio Diario SRAG\") as run:\n",
    "    log(\"Iniciando registro de auditoria no MLflow...\", \"INFO\")\n",
    "    \n",
    "    # 1. Logamos os Parâmetros\n",
    "    mlflow.log_param(\"data_execucao\", data_hora)\n",
    "    mlflow.log_param(\"modelo_usado\", \"llama-3-70b\")\n",
    "    \n",
    "    # 2. Log do Texto Final\n",
    "    nome_arquivo_relatorio = \"relatorio_srag.md\"\n",
    "    \n",
    "    # Pega a última mensagem válida do loop anterior\n",
    "    # (Nota: Assume que a variável 'message' ainda está na memória da célula anterior)\n",
    "    try:\n",
    "        texto_final = message.content \n",
    "        mlflow.log_text(texto_final, nome_arquivo_relatorio)\n",
    "        log(f\"Texto do relatório salvo: {nome_arquivo_relatorio}\", \"INFO\")\n",
    "    except NameError:\n",
    "        log(\"Variável 'message' não encontrada. O agente rodou?\", \"WARN\")\n",
    "    \n",
    "    # 3. Logamos os Gráficos. O MLflow copia as imagens do Volume para dentro do Experimento\n",
    "    try:\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_diario.png\", artifact_path=\"graficos\")\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_mensal.png\", artifact_path=\"graficos\")\n",
    "        log(\"Artefatos visuais (Gráficos) anexados ao experimento.\", \"SUCCESS\")\n",
    "    except Exception as e:\n",
    "        log(f\"Não foi possível logar as imagens: {e}\", \"WARN\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    log(\"Sucesso! Execução auditada no MLflow.\", \"SUCCESS\")\n",
    "    log(f\"Link para Experiment: {experiment_path}\", \"INFO\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "srag_agent_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}