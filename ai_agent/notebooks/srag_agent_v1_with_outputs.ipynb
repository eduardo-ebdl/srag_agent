{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef39dfe-0f9e-4b3b-af1b-4473f27a1162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instala√ß√£o das bibliotecas de Orquestra√ß√£o (LangChain/Graph), Conectores (Databricks) e Pesquisa (Tavily).\n",
    "%pip install -U langchain langchain-community langchain-core langgraph databricks-langchain tavily-python matplotlib pandas mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70594b0-22fb-4dc1-b2f9-89f02c6d02b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√µes\n",
    "\n",
    "# Bibliotecas padr√£o\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Dados e visualiza√ß√£o\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# LangChain e LLMs\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "# Servi√ßos externos e ML\n",
    "from tavily import TavilyClient\n",
    "import mlflow\n",
    "\n",
    "# Configura√ß√£o de Log\n",
    "def log(msg: str, level: str = \"INFO\"):\n",
    "    \"\"\"\n",
    "    Logger padronizado para rastreabilidade de execu√ß√£o.\n",
    "    Formato: [YYYY-MM-DD HH:MM:SS] [LEVEL] Icon Mensagem\n",
    "    \"\"\"\n",
    "    # Configura√ß√£o de Fuso Hor√°rio (BRT)\n",
    "    fuso_br = timezone(timedelta(hours=-3))\n",
    "    timestamp = datetime.now(fuso_br).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # √çcones visuais para facilitar leitura r√°pida dos logs\n",
    "    icons = {\n",
    "        \"INFO\": \"‚ÑπÔ∏è\", \n",
    "        \"WARN\": \"‚ö†Ô∏è\", \n",
    "        \"ERROR\": \"‚ùå\", \n",
    "        \"SUCCESS\": \"‚úÖ\", \n",
    "        \"SYSTEM\": \"‚öôÔ∏è\",\n",
    "        \"TOOL\": \"üõ†Ô∏è\",\n",
    "        \"AI\": \"ü§ñ\"\n",
    "    }\n",
    "    icon = icons.get(level, \"\")\n",
    "    \n",
    "    print(f\"[{timestamp}] [{level}] {icon} {msg}\")\n",
    "\n",
    "log(\"Bibliotecas importadas e Logger configurado com sucesso.\", \"SYSTEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf0b0c2-cbad-4f5a-8726-93052e2b8351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define o experimento\n",
    "\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/srag_agent_monitoring\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "# Habilita o rastreamento autom√°tico para LangChain (capturar inputs, outputs, traces)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "log(f\"MLOps Ativado. Experiment Path: {experiment_path}\", \"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c79805a-fffb-4e07-b9af-d7b20d7ca16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arquitetura dos dados\n",
    "\n",
    "CATALOG = \"srag_prod\" \n",
    "SCHEMA = \"gold\"       \n",
    "VOLUME_NAME = \"volume_imagens\" # Storage para arquivos n√£o estruturados (png)\n",
    "\n",
    "# Garante a exist√™ncia do volume para persist√™ncia dos gr√°ficos gerados pelo agente\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME_NAME}\")\n",
    "\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "TABLE_DAILY = f\"{CATALOG}.{SCHEMA}.gold_srag_daily\"\n",
    "TABLE_MONTHLY = f\"{CATALOG}.{SCHEMA}.gold_srag_monthly\"\n",
    "\n",
    "log(f\"Ambiente de Dados Configurado.\", \"SYSTEM\")\n",
    "log(f\"Caminho do Volume: {VOLUME_PATH}\", \"INFO\")\n",
    "log(f\"Tabelas Mapeadas: {TABLE_DAILY}, {TABLE_MONTHLY}\", \"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba5a11f-8e3e-4b63-9541-91e3e6397da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Valida√ß√£o dos dados\n",
    "\n",
    "def run_quality_gate():\n",
    "    \"\"\"\n",
    "    Valida os dados da tabela Gold. Se falhar, interrompe o notebook.\n",
    "    Regras:\n",
    "    1. Apenas anos 2024 e 2025.\n",
    "    2. Sem casos negativos.\n",
    "    3. Sem datas nulas.\n",
    "    \"\"\"\n",
    "    log(\"Executando Valida√ß√£o de Qualidade de Dados (Data Quality Gate)...\", \"SYSTEM\")\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # A fun√ß√£o 'sum' conta quantas linhas violam cada regra\n",
    "    query_check = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_linhas,\n",
    "            SUM(CASE WHEN year(data_referencia) NOT IN (2024, 2025) THEN 1 ELSE 0 END) as erro_anos,\n",
    "            SUM(CASE WHEN total_casos < 0 THEN 1 ELSE 0 END) as erro_negativos,\n",
    "            SUM(CASE WHEN data_referencia IS NULL THEN 1 ELSE 0 END) as erro_nulos\n",
    "        FROM {TABLE_DAILY}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Coleta o resultado\n",
    "    check = spark.sql(query_check).collect()[0]\n",
    "    \n",
    "    erros = []\n",
    "    \n",
    "    # 1. Valida√ß√£o de Anos (2024/2025 apenas)\n",
    "    if check['erro_anos'] > 0:\n",
    "        erros.append(f\"Regra de Ano: {check['erro_anos']} registros fora de 2024/2025.\")\n",
    "        \n",
    "    # 2. Valida√ß√£o de Negativos\n",
    "    if check['erro_negativos'] > 0:\n",
    "        erros.append(f\"Regra de Negativos: {check['erro_negativos']} registros com casos negativos.\")\n",
    "        \n",
    "    # 3. Valida√ß√£o de Nulos\n",
    "    if check['erro_nulos'] > 0:\n",
    "        erros.append(f\"Regra de Nulos: {check['erro_nulos']} registros com data vazia.\")\n",
    "        \n",
    "    # Verifica se houve algum erro\n",
    "    if erros:\n",
    "        msg_erro = \"\\n\".join(erros)\n",
    "        log(f\"FALHA CR√çTICA DE DATA QUALITY:\\n{msg_erro}\", \"ERROR\")\n",
    "        raise ValueError(f\"üö® O notebook foi interrompido para seguran√ßa dos dados.\")\n",
    "    \n",
    "    log(f\"Dados Aprovados: {check['total_linhas']} registros validados (2024-2025, Sem Nulos, Sem Negativos).\", \"SUCCESS\")\n",
    "\n",
    "# Roda a valida√ß√£o\n",
    "run_quality_gate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d34703f-4fe1-4a59-9a10-7965c2a23469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query e Teste Unit√°rio\n",
    "\n",
    "@tool\n",
    "def get_latest_srag_metrics() -> str:\n",
    "    \"\"\"\n",
    "    Consulta o banco de dados para obter as m√©tricas mais recentes de SRAG.\n",
    "    Retorna: Um JSON com total de casos, e taxas de mortalidade, UTI e vacina√ß√£o.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # log(\"Consultando m√©tricas no Lakehouse...\", \"TOOL\") # Opcional: Descomentar se quiser muito detalhe\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            cast(data_referencia as string) as data_referencia,\n",
    "            cast(total_casos as int) as total_casos,\n",
    "            cast(taxa_aumento_casos_perc as double) as taxa_aumento_casos_perc,\n",
    "            cast(taxa_mortalidade_perc as double) as taxa_mortalidade_perc,\n",
    "            cast(taxa_ocupacao_uti_perc as double) as taxa_ocupacao_uti_perc,\n",
    "            cast(taxa_vacinacao_pacientes_perc as double) as taxa_vacinacao_pacientes_perc\n",
    "        FROM {TABLE_DAILY}\n",
    "        ORDER BY data_referencia DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.sql(query).toPandas()\n",
    "        \n",
    "        if df.empty:\n",
    "            log(\"ALERTA: Tabela vazia ao buscar m√©tricas.\", \"WARN\")\n",
    "            return \"ALERTA: A tabela SQL retornou vazio. Verifique se o pipeline rodou.\"\n",
    "            \n",
    "        result = df.iloc[0].to_dict()\n",
    "        return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na Tool de M√©tricas: {str(e)}\", \"ERROR\")\n",
    "        return f\"ERRO CR√çTICO NO SPARK/SQL: {str(e)}\"\n",
    "\n",
    "# Teste manual imediato\n",
    "log(\"Teste Unit√°rio - M√©tricas:\", \"SYSTEM\")\n",
    "print(get_latest_srag_metrics.invoke({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f982fbb-f875-43ca-8115-be05fe6a660e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o\n",
    "\n",
    "@tool\n",
    "def generate_srag_charts() -> str:\n",
    "    \"\"\"\n",
    "    Gera gr√°ficos de linha (30 dias) e barras (12 meses) sobre SRAG.\n",
    "    Padr√£o de Data:\n",
    "    - Di√°rio: dd/mm (Ex: 28/12)\n",
    "    - Mensal: mm/aaaa (Ex: 12/2024) - Ano com 4 d√≠gitos para evitar confus√£o com dia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log(\"Iniciando gera√ß√£o de gr√°ficos...\", \"TOOL\")\n",
    "        \n",
    "        # GR√ÅFICO 1: DI√ÅRIO (30 DIAS) - Linha\n",
    "        query_daily = f\"SELECT data_referencia, total_casos FROM {TABLE_DAILY} ORDER BY data_referencia DESC LIMIT 30\"\n",
    "        df_daily = spark.sql(query_daily).toPandas().sort_values('data_referencia')\n",
    "        \n",
    "        path_daily = \"Sem dados di√°rios\"\n",
    "        if not df_daily.empty:\n",
    "            df_daily['data_referencia'] = pd.to_datetime(df_daily['data_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(df_daily['data_referencia'], df_daily['total_casos'], marker='o', color='#1f77b4', linewidth=2)\n",
    "            \n",
    "            # Formata√ß√£o Eixo X (Di√°rio) -> dd/mm\n",
    "            ax1 = plt.gca()\n",
    "            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator(interval=2)) \n",
    "            \n",
    "            plt.title('Casos SRAG - √öltimos 30 Dias')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_daily = f\"{VOLUME_PATH}/grafico_diario.png\"\n",
    "            plt.savefig(path_daily)\n",
    "            plt.close()\n",
    "\n",
    "        # GR√ÅFICO 2: MENSAL (12 MESES) - Barras\n",
    "        query_monthly = f\"SELECT mes_referencia, total_casos FROM {TABLE_MONTHLY} ORDER BY mes_referencia DESC LIMIT 12\"\n",
    "        df_monthly = spark.sql(query_monthly).toPandas().sort_values('mes_referencia')\n",
    "        \n",
    "        path_monthly = \"Sem dados mensais\"\n",
    "        if not df_monthly.empty:\n",
    "            df_monthly['mes_referencia'] = pd.to_datetime(df_monthly['mes_referencia'])\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(df_monthly['mes_referencia'], df_monthly['total_casos'], color='#ff7f0e', width=20)\n",
    "            \n",
    "            # Formata√ß√£o Eixo X (Mensal) -> mm/aaaa (Ex: 01/2025)\n",
    "            ax2 = plt.gca()\n",
    "            ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y')) \n",
    "            ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "            \n",
    "            plt.title('Casos SRAG - √öltimos 12 Meses')\n",
    "            plt.ylabel('Casos')\n",
    "            plt.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            path_monthly = f\"{VOLUME_PATH}/grafico_mensal.png\"\n",
    "            plt.savefig(path_monthly)\n",
    "            plt.close()\n",
    "\n",
    "        log(f\"Gr√°ficos persistidos no Volume: {path_daily}, {path_monthly}\", \"SUCCESS\")\n",
    "        return f\"Gr√°ficos atualizados salvos em:\\n1. {path_daily}\\n2. {path_monthly}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Falha ao gerar gr√°ficos: {e}\", \"ERROR\")\n",
    "        return f\"ERRO AO GERAR GR√ÅFICOS: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2529545-d1f7-418c-afb3-88c2144298b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contexto dados\n",
    "\n",
    "try:\n",
    "    token = dbutils.secrets.get(scope=\"my_srag_scope\", key=\"tavily_api_key\")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = token\n",
    "    log(\"API Key do Tavily carregada via Secrets.\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao carregar a API Key: {e}\", \"ERROR\")\n",
    "\n",
    "@tool\n",
    "def get_epidemiological_context() -> str:\n",
    "    \"\"\"\n",
    "    Realiza 'Grounding' (Ancoragem) do modelo em dados externos em tempo real.\n",
    "    Estrat√©gia:\n",
    "    1. Busca Macro (Global/OMS) para identificar novas variantes.\n",
    "    2. Busca Micro (Brasil/Fiocruz) para dados epidemiol√≥gicos locais.\n",
    "    Isso reduz a chance do modelo inventar contextos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        \n",
    "        # Passo 1: Contexto Internacional\n",
    "        log(\"Investigando cen√°rio Global (OMS/CDC)...\", \"TOOL\")\n",
    "        response_global = client.search(\n",
    "            query=\"Global respiratory virus trends WHO CDC influenza covid epidemiological update\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=2, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Passo 2: Contexto Nacional\n",
    "        log(\"Investigando cen√°rio Brasil (InfoGripe/Fiocruz)...\", \"TOOL\")\n",
    "        response_br = client.search(\n",
    "            query=\"Boletim InfoGripe Fiocruz Brasil cen√°rio atual SRAG covid influenza\", \n",
    "            search_depth=\"basic\", topic=\"news\", max_results=3, include_answer=True\n",
    "        )\n",
    "        \n",
    "        # Montagem do Contexto\n",
    "        contexto = f\"\"\"\n",
    "        | RELAT√ìRIO DE INTELIG√äNCIA EXTERNA |\n",
    "        1. GLOBAL: {response_global.get('answer', 'N/A')}\n",
    "        2. NACIONAL: {response_br.get('answer', 'N/A')}\n",
    "        \n",
    "        FONTES NACIONAIS:\n",
    "        \"\"\"\n",
    "        for res in response_br.get('results', []):\n",
    "            contexto += f\"- {res['title']}: {res['content'][:200]}...\\n\"\n",
    "            \n",
    "        return contexto\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"Erro na busca externa: {str(e)}\", \"ERROR\")\n",
    "        return f\"Erro na busca externa: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2119276c-3f38-443d-8d0f-51732c12170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defini√ß√£o do Agente\n",
    "\n",
    "# 1. Configura√ß√£o do Modelo: O par√¢metro 'model' define qual endpoint de infer√™ncia ser√° acionado.\n",
    "llm = ChatDatabricks(model=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "# 2. Binding das Ferramentas\n",
    "# O Agente precisa de uma lista de 'tools' dispon√≠veis para saber o que ele pode fazer.\n",
    "tools = [get_latest_srag_metrics, generate_srag_charts, get_epidemiological_context]\n",
    "\n",
    "# 3. Cria√ß√£o do Executor (runtime que pega o pensamento do LLM e efetivamente roda as ferramentas Python).\n",
    "try:\n",
    "    agent_executor = create_agent(llm, tools)\n",
    "    log(f\"Agente configurado e pronto! Modelo: {llm.model}\", \"SUCCESS\")\n",
    "except Exception as e:\n",
    "    log(f\"Erro ao criar agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69fc1f5-0112-443e-a7b0-40fe031fa9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execu√ß√£o final\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# 1. Configura√ß√£o de Data/Hora\n",
    "fuso_br = timezone(timedelta(hours=-3))\n",
    "data_hora = datetime.now(fuso_br).strftime(\"%d/%m/%Y √†s %H:%M\")\n",
    "\n",
    "# 2. Defini√ß√£o do System Prompt\n",
    "system_instructions = \"\"\"\n",
    "ATEN√á√ÉO: Voc√™ √© um Engenheiro de IA S√™nior. Siga este roteiro ESTRITAMENTE.\n",
    "\n",
    "--- ORDEM DE EXECU√á√ÉO (OBRIGAT√ìRIA) ---\n",
    "1. FERRAMENTAS INTERNAS: Chame `get_latest_srag_metrics` e `generate_srag_charts`.\n",
    "2. FERRAMENTA EXTERNA: Chame `get_epidemiological_context`.\n",
    "   - üõë PARE E ESPERE O RETORNO DA FERRAMENTA.\n",
    "   - Use o texto retornado para compor as se√ß√µes Global e Brasil.\n",
    "\n",
    "--- üö® CORRE√á√ÉO DE LINGUAGEM (LEIA COM ATEN√á√ÉO) üö® ---\n",
    "O campo do banco chama-se \"taxa_aumento\", mas valores negativos representam QUEDA.\n",
    "Voc√™ est√° PROIBIDO de dizer \"taxa de aumento negativa\".\n",
    "\n",
    "‚ùå ERRADO (NUNCA ESCREVA ASSIM):\n",
    "\"O volume de casos recuou, com uma taxa de aumento de casos de -10.47%.\"\n",
    "\"Houve um aumento de -10%.\"\n",
    "\n",
    "‚úÖ CERTO (ESCREVA ASSIM):\n",
    "\"O volume de novos casos **registrou uma queda de 10,47%**.\"\n",
    "\"Houve uma **redu√ß√£o de 10,47%** nas notifica√ß√µes.\"\n",
    "\"Os casos **diminu√≠ram 10,47%** em rela√ß√£o ao per√≠odo anterior.\"\n",
    "\n",
    "--- ESTRUTURA DO RELAT√ìRIO ---\n",
    "   ## üìä An√°lise dos Dados Internos (DataSUS)\n",
    "   (Texto fluido aplicando a regra do \"CERTO\" acima. Cite os n√∫meros de mortalidade, UTI e vacina√ß√£o. Ao final, informe apenas: \"Gr√°ficos de tend√™ncia anexados ao volume.\")\n",
    "   \n",
    "   ## üåç Panorama Global\n",
    "   (Resumo direto das tend√™ncias da OMS/Mundo trazidas pela tool).\n",
    "   \n",
    "   ## üáßüá∑ Cen√°rio Brasil (InfoGripe/Fiocruz)\n",
    "   (Resumo direto do cen√°rio nacional trazido pela tool).\n",
    "\n",
    "   ## üöÄ Conclus√£o T√©cnica\n",
    "   (Recomenda√ß√£o final curta).\n",
    "\"\"\"\n",
    "\n",
    "log(f\"Iniciando ciclo de an√°lise do Agente...\", \"SYSTEM\")\n",
    "\n",
    "# 3. Montagem do Payload\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=system_instructions),\n",
    "        # Aqui entra o PROMPT DO USU√ÅRIO\n",
    "        (\"user\", \"Gere o relat√≥rio de monitoramento SRAG de hoje.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Execu√ß√£o com Logs Detalhados\n",
    "try:\n",
    "    for chunk in agent_executor.stream(inputs, stream_mode=\"values\", config={\"recursion_limit\": 25}):\n",
    "        message = chunk[\"messages\"][-1]\n",
    "        \n",
    "        if message.type == \"ai\" and not message.tool_calls:\n",
    "            # Mostra o pensamento ou a resposta final\n",
    "            print(f\"\\n--- [AGENTE PENSANDO / RESPOSTA] ---\\n{message.content}\")\n",
    "        elif message.type == \"tool\":\n",
    "            log(f\"Ferramenta acionada: {message.name}\", \"TOOL\")\n",
    "    \n",
    "    hora_fim = datetime.now(fuso_br).strftime(\"%H:%M:%S\")\n",
    "    print(\"\\n\" + (\"=\" * 50))\n",
    "    log(f\"Processo finalizado √†s {hora_fim}\", \"SUCCESS\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Erro na execu√ß√£o do loop do agente: {e}\", \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0633c2ef-42c1-41d4-97bc-e2044dd49461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registro de Execu√ß√£o\n",
    "\n",
    "# Salvar o resultado do agente\n",
    "with mlflow.start_run(run_name=\"Relatorio Diario SRAG\") as run:\n",
    "    log(\"Iniciando registro de auditoria no MLflow...\", \"INFO\")\n",
    "    \n",
    "    # 1. Logamos os Par√¢metros\n",
    "    mlflow.log_param(\"data_execucao\", data_hora)\n",
    "    mlflow.log_param(\"modelo_usado\", \"llama-3-70b\")\n",
    "    \n",
    "    # 2. Log do Texto Final\n",
    "    nome_arquivo_relatorio = \"relatorio_srag.md\"\n",
    "    \n",
    "    # Pega a √∫ltima mensagem v√°lida do loop anterior\n",
    "    # (Nota: Assume que a vari√°vel 'message' ainda est√° na mem√≥ria da c√©lula anterior)\n",
    "    try:\n",
    "        texto_final = message.content \n",
    "        mlflow.log_text(texto_final, nome_arquivo_relatorio)\n",
    "        log(f\"Texto do relat√≥rio salvo: {nome_arquivo_relatorio}\", \"INFO\")\n",
    "    except NameError:\n",
    "        log(\"Vari√°vel 'message' n√£o encontrada. O agente rodou?\", \"WARN\")\n",
    "    \n",
    "    # 3. Logamos os Gr√°ficos. O MLflow copia as imagens do Volume para dentro do Experimento\n",
    "    try:\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_diario.png\", artifact_path=\"graficos\")\n",
    "        mlflow.log_artifact(f\"{VOLUME_PATH}/grafico_mensal.png\", artifact_path=\"graficos\")\n",
    "        log(\"Artefatos visuais (Gr√°ficos) anexados ao experimento.\", \"SUCCESS\")\n",
    "    except Exception as e:\n",
    "        log(f\"N√£o foi poss√≠vel logar as imagens: {e}\", \"WARN\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    log(\"Sucesso! Execu√ß√£o auditada no MLflow.\", \"SUCCESS\")\n",
    "    log(f\"Link para Experiment: {experiment_path}\", \"INFO\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "srag_agent_v1_with_outputs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
